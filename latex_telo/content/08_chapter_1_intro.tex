\section{Introduction}

This document was created in the context of the Computational Mathematics Seminary
at the Technical University of Vienna (SE - 3ECTS). In this introduction, the scientific relevance
of the work is highlighted and a brief overview of the topics covered is given.\\

PDE Constrained Shape Optimization is a topic of interest in almost all engineering fields
where the relevant phenomena can be described by Partial Differential Equations (PDEs) and
an optimization problem can be formulated. Here, the stationary linear stokes equations are
the PDEs and the energy dissipation over the domain is optimized. The optimization can 
be described by a minimization problem which can be solved with the gradient descent method.
Its important to note, that the PDE constraints and optimization goals used here, 
can be exchanged with arbitrary PDE constraints and optimization goals. Some parts,
especially the proof for shape derrivative existence, can get more complex for e.g. 
Non-Linear PDEs or Transient PDEs.\\

\subsection*{NGSolve}
The practical implementation part is done in NGSolve, which is a object oriented python Finite Element
Method library with automatic differentiation capabilites. The mathematical formulations in this document 
can mostly be implemented directly, as one can see in either in appendix APPENDIX or in the associated
Jupyter Notebook file. Gangl et. al. \cite{fully_semi_paper_sturm} have shown the PDE Constrained 
Optimization capabilites of NGSolve. For more in-depth explanations and examples on NGSolve 
visit \href{https://ngsolve.org/}{ngsolve.org}.

\subsection*{Minimization Problem}
A generic PDE constrained optimization problem is of the following form:
\begin{align*}
        \min_{ \Omega \in \mathcal{A} } J( \Omega, u) \\
        s.t. \quad \mathrm{B}_{\Omega}(u) = 0
\end{align*}
Where $\Omega$ is the Domain for the PDE, $\mathcal{A}$ is the set of admissible shapes
$J(\Omega, u)$ is a functional that is to be minimized and $\mathrm{B}_{\Omega}(u)$ is the PDE
constraint and its solution $u$. The Domain $\Omega$ is what is going to be 
optimized in the underlying work. \\

\subsection*{Shape Derrivative}
In order to find a numerical solution to the minimization problem with the gradient descent
method, the existence of the analytical shape derrivative needs to be shown. Here the
differentiability of $J(\Omega,u)$ at $\Omega \in \mathcal{A}$ in direction $X$ is shown. 
As Sturm et. al. \cite{nearly_conformal_paper} have shown, the functional 
$J(\Omega, u)$ can be reduced to a functional $J(\Omega)$ and the shape derrivative 
$\mathrm{d}J(\Omega)(X)$ exists. In chapter \ref*{}, the proof is recapitulated briefly.

\pagebreak

\subsection*{Auxiliary Problem - Descent Direction}
To find the gradient descent direction, here the vectorfield $-X$, an auxiliary problem needs to be solved.
Since its solved with the Finite Element Method libary NGSolve, the PDE problem is posed in a weak sense where 
$H$ is e.g. a Sobolov space. Find $X \in [H(\Omega)]^2:$ 
\begin{align*}
    \mathrm{d}J(\Omega)(X) = \mathrm{b}(X,\varphi)_H \quad \forall  \varphi \in H
\end{align*}
If the bi-linear form $\mathrm{b}(.,.)_H$ is chosen such that it is positive definite, the negative solution $X$
of the auxiliary problem points in the negative direction of the gradient.

\subsection*{Optimization Steps}
The problem $\mathrm{B}_{\Omega}(u) = 0$ can be solved, the shape derivative $\mathrm{d}J(\Omega)(X)$ can be calculated, the auxilary problem which yields 
the descent direction $-X$ can be solved as well. In the last step the optimization takes place where one can e.g. use a gradient descent method
 from \texttt{numpy}.
 \begin{align*}
    X \in [C^{0,1}(\Omega)]^2 \quad , \, \mathrm{T}_t(.):= \mathrm{id} + tX \quad , \, \text{choose } t \in \mathbb{R}
\end{align*}
Since we chose the gradient descent direction to point in $-X$ direction, the following must hold true for the energy dissipation functional:
\begin{align*}
    J(\mathrm{T}_t(\Omega)) <J(\Omega)
\end{align*}
\subsection*{Penalty Method}
If the differentiability of the shape functional has been shown, a method to obtain a numerical result to the minimization problem,
is by introducing an Augmented Lagrangian, here briefly discussed is the underlying work used Quadratic Penalty Method. Again one considers:
\begin{align*}
    \min_{ \Omega} J( \Omega) \quad \text{s.t.} \quad \mathrm{B}_{\Omega}(u) = 0
\end{align*}
where $J(.) : \mathbb{R}^n \rightarrow \mathbb{R}$, and $\mathrm{B}_{\Omega}(.):\mathbb{R}^n \rightarrow \mathbb{R}^p$ are differentiable functions.
If the differentiability has been shown, the quadratic penalty method yields an approximative minimization result:
\begin{align*}
    \mathcal{L}_{\alpha}(\Omega) = J(\Omega) + \frac{\alpha}{2}|\mathrm{B}_{\Omega}(u)|^2 \, , \quad \alpha > 0s
\end{align*}
For more in depth elaborations on Penalty Methods and Augmented Lagrangian, 
see Numerical Optimization Lecture Notes from Dr. K. Sturm \cite{lecture_notes_sturm}. This Augmented Lagrangian can now be derived 
and used for the step direction $-X$.